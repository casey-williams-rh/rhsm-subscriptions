# This file defines properties that are common to all swatch services.
# import via:
# spring:
#   config:
#     import: classpath:swatch-core/application.yaml

# Variables derived from Clowder should have their defaults defined here. Failure to do so will
# result in an exception at start-up when the clowder property cannot be successfully resolved.
# The values can still be overridden using environment variables as they have a higher precedence
# than the defaults defined here.
KAFKA_BOOTSTRAP_HOST: ${clowder.kafka.brokers[0].hostname:localhost}
KAFKA_BOOTSTRAP_PORT: ${clowder.kafka.brokers[0].port:9094}
KAFKA_USERNAME: ${clowder.kafka.brokers[0].sasl.username:client}
KAFKA_PASSWORD: ${clowder.kafka.brokers[0].sasl.password:dummy}
KAFKA_SASL_PROTOCOL: ${clowder.kafka.brokers[0].sasl.securityProtocol:SASL_SSL}
KAFKA_SASL_MECHANISM: ${clowder.kafka.brokers[0].sasl.saslMechanism:PLAIN}

# This needs to be a PEM encoded certificate string.  If you want to point to a file you can use
# spring.kafka.ssl.trust-store-location property but that property is mutually exclusive with the
# spring.kafka.ssl.trust-store-certificates property.  Since Clowder gives us the actual
# certificate PEM, we have to use the trust-store-certificates property.  The ideal way to do
# this would be to use profiles correctly and have a dev profile that has trust-store-location
# defined and a prod profile that has trust-store-certificates defined.
#
# The value of this property must match a regular expression defined in Kafka's
# DefaultSslEngineFactory.PemParser class.  The regular expression is (with Java escapes)
# -+BEGIN\\s*.*CERTIFICATE[^-]*-+\\s+(?:\\s*[^\\r\\n]*:[^\\r\\n]*[\\r\\n]+)*([a-zA-Z0-9/+=\\s]*)-+END\\s*.*CERTIFICATE[^-]*-+\\s+")
# A notable point about this expression is that the "----BEGIN CERTIFICATE-----" piece must be
# followed by a newline which makes it challenging to place the value in a properties file.  The
# output from the following command will create a properly formatted value for a properties file
# (as opposed to this YAML file):
# cat <(head -n1 config/kafka/test-ca.crt) <(head -n -1 config/kafka/test-ca.crt | tail -n +2 | sed -z 's^\n^\\\n^g') <(tail -n1 config/kafka/test-ca.crt) | sed -z 's^\n^\\n\\\n^'
KAFKA_SSL_TRUSTSTORE_CERTIFICATE: ${clowder.kafka.brokers[0].cacert:-----BEGIN CERTIFICATE-----
  MIIFBTCCAu2gAwIBAgIUI8W01ruDKw/0ZF2IS9hDww8KeXowDQYJKoZIhvcNAQEL
  BQAwEjEQMA4GA1UEAwwHVGVzdCBDQTAeFw0yMjA3MTQxODAwNTVaFw0zMjA3MTEx
  ODAwNTVaMBIxEDAOBgNVBAMMB1Rlc3QgQ0EwggIiMA0GCSqGSIb3DQEBAQUAA4IC
  DwAwggIKAoICAQCbu+Yiyqv+r+wu2DV+xCptmHz9Vtg5eT+5gTyYhf2Ycfq7xbFD
  BnY1a7LQ5naWxsJQNSFicBoroHZWjt4lOO1ov8Zb6aIE/ZqLbpbYdli1UpGswCAT
  a9vkLrh9p/WeSHQftyL8HQDBuK61jqzw2xShR6USIAmUEENp8mP0sFYNJIsHofHP
  ERlx5cZj78vBpJgMUlcHzifv+z6VJfQ63k2Eth/zcl94zD6fa+uHlOyzx8OCB7EC
  wa5GVJzKYc8x5bxJi3TvESCntPb+mFve+h2Md00sQwVRWW3BR4iJMm7aS6A2s6Q+
  H0qd9Wq9FhpLykmHc6tQnuwR40DusmXTMcPnzJrWcwV4V8DN0bPH2z94QV4aZ/iO
  NkN2DCOr/Ze8BMZnLYtbJxHU9xzMZj54l9ngdT8H0EmMZYdmQH39MUgI1J9u7C/v
  fqLMLOc9daPy0dbJgFud/s1ZTgAzX2VITv+yXMKDFTaKHEutA2j4U1Vucfp1EnfY
  pDTsbJ2oYVhoXK869mu7OfByjQKTxQFGl45iXl50s7dsLGUy5r/W6+iL3zZGDkU/
  ZH6X/Ujay3VmoekVEyBAf6WAKM36Sp/va/GhlIEQI/EwQdg/XCoRTbDhLbbkVcPA
  RdNL0Yxh6kyvFZOZCcyXBZNR4ykltIdMkFAAIgpY2DEnkrepSGytm/2gtQIDAQAB
  o1MwUTAdBgNVHQ4EFgQUQUgWjSm2SiPfirhYZSlUzVHuJf8wHwYDVR0jBBgwFoAU
  QUgWjSm2SiPfirhYZSlUzVHuJf8wDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0B
  AQsFAAOCAgEAO1WgnbRrf5uSh8Op1Xp8LC6C/4rQJP1d1zOL1QtaJ/Nh5Z0GCD9x
  Y6iRqYZPl6Udj9/Y4H/TVuz2c6oNy0ZfmiENzOo21iVR+BjAbxq1ROLY3ZrSNwnR
  3nPISqQ+9bVmGjsX3T+K+Mu5na6N5y+9LvtIKhDOPXPW5LvjF39bX3J9qtsw15zw
  uUzu4/qeLWxTdDPuZPOk7L2ydMdYp/E+QCzBQ4hW2pj4hyvPR/DQ7t3mYH0PWf3M
  u6eQAePnfVxa0Lqm8XIc3Sl7b2leHvEDAwUDeQ2zT/CUtrnLHregEj89NjM21Efk
  zLprcKbY1+t+6OGcrkimEb++6YKAllKcgGyat5wKDyrSGGwvvZaDKp1IjjxEpVIR
  1rnw6O91kbyvMBdYuPCu55OwxJN/UOfjuQMHhwoouLNV90yYm5uwaWTV360+YgfA
  T25JEmalZRLfNeepuNq1pc34YAEGhHASduw+Q4uXUzeJxiveTR8Owar3DIlmXSye
  QoMAB9QqQbU3jh+pFCiUctUA/MT3dSEOEWSeGgBtykvKZvdxQ8/iPvdAkfd5vU7q
  nuFft1VQZOeVuIpH3kpv9+iROnX4W+iNO+omT5NcazTthJT3UohDDXvQCILJkl+e
  UfNfHwZGN4T/GekEp85CL2VcvM+Gx0gy3pf0IhlyzALCODwyoaZMno4=
  -----END CERTIFICATE-----
  }

SERVER_PORT: ${clowder.publicPort:8000}

# Values assigned to management.path-mapping.* shouldn't have a leading slash. However, Clowder
# only provides a path starting with a leading slash.  I have elected to set the default to do the
# same for the sake of consistency.  The leading slash can potentially cause problems with Spring
# Security since the path now becomes (assuming management.base-path is "/") "//metrics".  Browser
# requests to "/metrics" aren't going to match according to Spring Security's path matching rules
# and the end result is that any security rule applied to EndpointRequest.to("prometheus") will be
# applied to the defined path ("//metrics") rather than the de facto path ("/metrics").
#
# Accordingly, I've put in a custom rule in the security config to grant access to "/metrics"
METRICS_PROMETHEUS_PATH: ${clowder.metricsPath:/metrics}
METRICS_BASE_PATH: /
METRICS_SERVER_PORT: ${clowder.metricsPort:9000}

JMX_ENABLED: true
JDBC_BATCH_SIZE: 100
KAFKA_MESSAGE_THREADS: 1
KAFKA_IDLE_EVENT_INTERVAL: 5s
KAFKA_CONSUMER_MAX_POLL_INTERVAL_MS: 1800000
KAFKA_CONSUMER_RECONNECT_BACKOFF_MS: 2000
KAFKA_CONSUMER_RECONNECT_BACKOFF_MAX_MS: 10000
KAFKA_API_RECONNECT_TIMEOUT_MS: 480000

HAWTIO_DISABLE_PROXY: true
HAWTIO_AUTHENTICATION_ENABLED: false
HAWTIO_PROXY_ALLOWLIST: localhost,127.0.0.1
HAWTIO_LOCAL_ADDRESS_PROBING: true
HAWTIO_BASE_PATH:

DEV_MODE: false
DEVTEST_SUBSCRIPTION_EDITING_ENABLED: true
DEVTEST_EVENT_EDITING_ENABLED: false
ENABLE_ACCOUNT_RESET: false
PATH_PREFIX: api
APP_NAME: rhsm-subscriptions

DATABASE_HOST: localhost
DATABASE_PORT: 5432
DATABASE_DATABASE: rhsm-subscriptions
DATABASE_SSL_MODE: disable
DATABASE_SSL_CERT: /dev/null
DATABASE_USERNAME: rhsm-subscriptions
DATABASE_PASSWORD: rhsm-subscriptions
DATABASE_CONNECTION_TIMEOUT_MS: 30000
DATABASE_MAX_POOL_SIZE: 10

server:
  port: ${SERVER_PORT}

management:
  server:
    port: ${METRICS_SERVER_PORT}
  endpoints:
    web:
      exposure:
        include:
          - hawtio
          - health
          - info
          - jolokia
          - prometheus
      path-mapping:
        prometheus: ${METRICS_PROMETHEUS_PATH}
      base-path: ${METRICS_BASE_PATH}
  endpoint:
    shutdown:
      enabled: true
    prometheus:
      enabled: true
    # The liveness and readiness probes are enabled automatically when Spring Boot detects
    # kubernetes environment variables.  This setting just enables them always so that we see them
    # when running in a local deployment.
    health:
      probes:
        enabled: true

spring:
  jmx:
    enabled: ${JMX_ENABLED}
  # general hibernate configurations
  jpa:
    properties:
      hibernate:
        jdbc:
          batch_size: ${JDBC_BATCH_SIZE}
        order_inserts: true
        order_updates: true
  liquibase:
    change-log: classpath:/liquibase/changelog.xml
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_HOST}:${KAFKA_BOOTSTRAP_PORT}
    listener:
      # The number of threads that will be processing messages (should match
      # the number of partitions on the queue)
      concurrency: ${KAFKA_MESSAGE_THREADS}
      idle-event-interval: ${KAFKA_IDLE_EVENT_INTERVAL}
    consumer:
      properties:
        # Required kafka defaults
        max.poll.interval.ms: ${KAFKA_CONSUMER_MAX_POLL_INTERVAL_MS}
        reconnect.backoff.ms: ${KAFKA_CONSUMER_RECONNECT_BACKOFF_MS}
        reconnect.backoff.max.ms: ${KAFKA_CONSUMER_RECONNECT_BACKOFF_MAX_MS}
        default.api.timeout.ms: ${KAFKA_API_RECONNECT_TIMEOUT_MS}
      # if no offset commit exists yet, set to earliest
      auto-offset-reset: earliest
      max-poll-records: 1
    security:
      protocol: ${KAFKA_SASL_PROTOCOL}
    jaas:
      enabled: true
      # The value for loginModule is determined at runtime by the KafkaJaasBeanPostProcessor.
      # The value for the property is dependent on KAFKA_SASL_MECHANISM but isn't the literal text;
      # instead, it's a Kafka class name to use.
      # loginModule: One of [PLAIN, SCRAM-SHA-512, SCRAM-SHA-256]
      # All the loginModules we currently support use the same options.  If that changes, the
      # KafkaJaasBeanPostProcessor will need updating.
      options:
        username: ${KAFKA_USERNAME}
        password: ${KAFKA_PASSWORD}
    ssl:
      trust-store-certificates: ${KAFKA_SSL_TRUSTSTORE_CERTIFICATE}
      trust-store-type: PEM
    properties:
      sasl:
        mechanism: ${KAFKA_SASL_MECHANISM}

# See https://hawt.io/docs/configuration/ for details on built-in hawtio config
hawtio:
  # disable the remote connection tab, we do not need it
  disableProxy: ${HAWTIO_DISABLE_PROXY}
  authenticationEnabled: ${HAWTIO_AUTHENTICATION_ENABLED}
  proxyAllowlist: ${HAWTIO_PROXY_ALLOWLIST}
  localAddressProbing: ${HAWTIO_LOCAL_ADDRESS_PROBING}
  # Base path override for reverse proxy support
  hawtio-base-path: ${HAWTIO_BASE_PATH}

rhsm-subscriptions:
  security:
    dev-mode: ${DEV_MODE}
    manual-subscription-editing-enabled: ${DEVTEST_SUBSCRIPTION_EDITING_ENABLED}
    manual-event-editing-enabled: ${DEVTEST_EVENT_EDITING_ENABLED}
    reset-account-enabled: ${ENABLE_ACCOUNT_RESET}
  package_uri_mappings:
    # this mapping required here because it is used by our SecurityConfig, which is shared
    org.candlepin.subscriptions.resteasy: ${PATH_PREFIX}/${APP_NAME}/v1
  datasource:
    url: jdbc:postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_DATABASE}?reWriteBatchedInserts=true&stringtype=unspecified&sslmode=${DATABASE_SSL_MODE}&sslrootcert=${DATABASE_SSL_CERT}
    username: ${DATABASE_USERNAME}
    password: ${DATABASE_PASSWORD}
    driver-class-name: org.postgresql.Driver
    platform: postgresql
    hikari:
      connection-timeout: ${DATABASE_CONNECTION_TIMEOUT_MS}
      maximum-pool-size: ${DATABASE_MAX_POOL_SIZE}
  product:
    useStub: ${PRODUCT_USE_STUB:false}
    url: ${PRODUCT_URL:https://product.qa.api.redhat.com/svcrest/product/v3}
    keystore: file:${PRODUCT_KEYSTORE:}
    keystorePassword: ${PRODUCT_KEYSTORE_PASSWORD:redhat}
    maxConnections: ${PRODUCT_MAX_CONNECTIONS:100}
    tasks:
      topic: platform.rhsm-subscriptions.offering-sync
      kafka-group-id: offering-worker
  auth:
    swatchPsks:
      self: ${SWATCH_SELF_PSK:placeholder}
